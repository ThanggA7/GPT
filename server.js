import express from "express";
import cors from "cors";
import dotenv from "dotenv";
import path from "path";
import { fileURLToPath } from "url";

dotenv.config();

const app = express();
app.use(express.json({ limit: "25mb" }));
app.use(cors()); // Enable CORS for all routes

const __filename = fileURLToPath(import.meta.url);
const __dirname = path.dirname(__filename);
app.use(express.static(path.join(__dirname, "public")));

const GEMINI_API_KEY = process.env.GEMINI_API_KEY;
const GEMINI_API_URL =
  "https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent";

if (!GEMINI_API_KEY) {
  console.error("‚ùå Thi·∫øu GEMINI_API_KEY trong .env");
  process.exit(1);
}

const PORT = process.env.PORT || 5500;

app.options("/api/chat", (req, res) => res.sendStatus(204));

app.post("/api/chat", async (req, res) => {
  console.log("üì® Received chat request:", {
    hasMessage: !!req.body?.message,
    imageCount: req.body?.images?.length || 0,
    temperature: req.body?.temperature,
    timestamp: new Date().toISOString(),
  });

  // Set timeout for the entire request
  const timeoutId = setTimeout(() => {
    if (!res.headersSent) {
      console.error("‚è∞ Request timeout after 30 seconds");
      res.status(408).json({
        error: "Y√™u c·∫ßu h·∫øt th·ªùi gian ch·ªù",
        details: "AI ƒëang qu√° t·∫£i, vui l√≤ng th·ª≠ l·∫°i sau √≠t ph√∫t",
      });
    }
  }, 30000);

  try {
    const {
      message,
      images = [],
      temperature = 0.7,
      conversationHistory = [],
    } = req.body || {};

    // Validate input - message should be non-empty string or images should be provided
    const hasMessage =
      message && typeof message === "string" && message.trim().length > 0;
    const hasImages = images && Array.isArray(images) && images.length > 0;

    if (!hasMessage && !hasImages) {
      clearTimeout(timeoutId);
      return res.status(400).json({
        error: "Message or images required",
        details: "Please provide either a text message or upload images",
      });
    }

    console.log(
      `üí≠ Processing request with ${conversationHistory.length} previous messages`
    );

    // Build conversation contents for Gemini
    const contents = [];

    // Add conversation history (limit to last 20 messages to avoid token limit)
    const recentHistory = conversationHistory.slice(-20);
    for (const historyItem of recentHistory) {
      if (historyItem.role === "user") {
        contents.push({
          role: "user",
          parts: [{ text: historyItem.content }],
        });
      } else if (historyItem.role === "assistant") {
        contents.push({
          role: "model", // Gemini uses 'model' instead of 'assistant'
          parts: [{ text: historyItem.content }],
        });
      }
    }

    // Add current message parts
    const currentParts = [];

    // Add text message if provided
    if (hasMessage) {
      currentParts.push({ text: message.trim() });
    }

    // Add images if provided
    if (hasImages) {
      for (const image of images) {
        currentParts.push({
          inline_data: {
            mime_type: image.mimeType || "image/jpeg",
            data: image.data.split(",")[1], // Remove data:image/jpeg;base64, prefix
          },
        });
      }
    }

    // Add current user message
    contents.push({
      role: "user",
      parts: currentParts,
    });

    const requestBody = {
      contents: contents,
      generationConfig: {
        temperature: Math.max(0, Math.min(1, temperature)),
        maxOutputTokens: 4096,
        topP: 0.8,
        topK: 40,
      },
      safetySettings: [
        {
          category: "HARM_CATEGORY_HARASSMENT",
          threshold: "BLOCK_MEDIUM_AND_ABOVE",
        },
        {
          category: "HARM_CATEGORY_HATE_SPEECH",
          threshold: "BLOCK_MEDIUM_AND_ABOVE",
        },
        {
          category: "HARM_CATEGORY_SEXUALLY_EXPLICIT",
          threshold: "BLOCK_MEDIUM_AND_ABOVE",
        },
        {
          category: "HARM_CATEGORY_DANGEROUS_CONTENT",
          threshold: "BLOCK_MEDIUM_AND_ABOVE",
        },
      ],
    };

    console.log(
      "ü§ñ Calling Gemini API with",
      hasImages ? "text + images" : "text only"
    );

    // Retry logic with exponential backoff
    let lastError = null;
    let response = null;

    for (let attempt = 1; attempt <= 3; attempt++) {
      try {
        console.log(`üîÑ API attempt ${attempt}/3`);

        const controller = new AbortController();
        const apiTimeout = setTimeout(() => controller.abort(), 25000); // 25 second timeout per attempt

        response = await fetch(`${GEMINI_API_URL}?key=${GEMINI_API_KEY}`, {
          method: "POST",
          headers: {
            "Content-Type": "application/json",
          },
          body: JSON.stringify(requestBody),
          signal: controller.signal,
        });

        clearTimeout(apiTimeout);

        if (response.ok) {
          console.log(`‚úÖ API call successful on attempt ${attempt}`);
          break; // Success, exit retry loop
        } else {
          const errorText = await response.text();
          console.error(
            `‚ùå API attempt ${attempt} failed with status ${response.status}:`,
            errorText
          );
          lastError = new Error(`HTTP ${response.status}: ${errorText}`);

          // Don't retry on client errors (4xx), only server errors (5xx)
          if (response.status < 500) {
            throw lastError;
          }
        }
      } catch (error) {
        console.error(`‚ùå API attempt ${attempt} error:`, error.message);
        lastError = error;

        // Don't retry on abort errors or client errors
        if (error.name === "AbortError" || error.message.includes("400")) {
          throw error;
        }

        // Wait before retry (exponential backoff)
        if (attempt < 3) {
          const waitTime = Math.pow(2, attempt - 1) * 1000; // 1s, 2s, 4s
          console.log(`‚è≥ Waiting ${waitTime}ms before retry...`);
          await new Promise((resolve) => setTimeout(resolve, waitTime));
        }
      }
    }

    if (!response || !response.ok) {
      throw lastError || new Error("All API attempts failed");
    }

    const data = await response.json();

    if (!data.candidates || data.candidates.length === 0) {
      console.error("‚ùå No candidates in response:", data);
      throw new Error(
        "AI kh√¥ng th·ªÉ t·∫°o ph·∫£n h·ªìi, c√≥ th·ªÉ do n·ªôi dung b·ªã h·∫°n ch·∫ø"
      );
    }

    const candidate = data.candidates[0];

    // Check for safety filters
    if (candidate.finishReason === "SAFETY") {
      return res.status(400).json({
        error: "N·ªôi dung b·ªã ch·∫∑n do vi ph·∫°m ch√≠nh s√°ch an to√†n",
        safetyRatings: candidate.safetyRatings,
      });
    }

    // Check for other finish reasons
    if (candidate.finishReason === "RECITATION") {
      return res.status(400).json({
        error: "N·ªôi dung b·ªã ch·∫∑n do vi ph·∫°m b·∫£n quy·ªÅn",
      });
    }

    if (!candidate.content?.parts?.[0]?.text) {
      console.error("‚ùå Invalid response structure:", data);
      return res.status(500).json({
        error: "Ph·∫£n h·ªìi kh√¥ng h·ª£p l·ªá t·ª´ AI",
        finishReason: candidate.finishReason,
      });
    }

    const aiResponse = candidate.content.parts[0].text;

    console.log("‚úÖ Sending response, length:", aiResponse.length);
    clearTimeout(timeoutId); // Clear timeout on success

    res.status(200).json({
      message: aiResponse,
      finishReason: candidate.finishReason,
      safetyRatings: candidate.safetyRatings,
    });
  } catch (error) {
    console.error("‚ùå Server error:", error);
    clearTimeout(timeoutId); // Clear timeout on error

    // Don't send response if already sent (timeout case)
    if (res.headersSent) {
      return;
    }

    let errorMessage = "Xin l·ªói, t√¥i kh√¥ng nh·∫≠n ƒë∆∞·ª£c ph·∫£n h·ªìi t·ª´ AI.";
    let statusCode = 500;

    if (error.name === "AbortError" || error.message.includes("timeout")) {
      errorMessage = "AI ƒëang qu√° t·∫£i, vui l√≤ng th·ª≠ l·∫°i sau √≠t gi√¢y";
      statusCode = 408;
    } else if (error.message.includes("API key")) {
      errorMessage = "L·ªói c·∫•u h√¨nh API key";
      statusCode = 401;
    } else if (
      error.message.includes("quota") ||
      error.message.includes("limit")
    ) {
      // Instead of returning error, provide demo response
      console.log("üìù API quota exceeded, providing demo response");
      const demoResponse = getDemoResponse(message || "");
      return res.json({ response: demoResponse.text });
    } else if (
      error.message.includes("network") ||
      error.message.includes("fetch")
    ) {
      errorMessage = "L·ªói k·∫øt n·ªëi m·∫°ng, vui l√≤ng th·ª≠ l·∫°i";
      statusCode = 503;
    } else if (
      error.message.includes("b·ªã h·∫°n ch·∫ø") ||
      error.message.includes("candidates")
    ) {
      errorMessage =
        "N·ªôi dung c√≥ th·ªÉ vi ph·∫°m ch√≠nh s√°ch, vui l√≤ng th·ª≠ c√¢u h·ªèi kh√°c";
      statusCode = 400;
    }

    res.status(statusCode).json({
      error: errorMessage,
      details:
        process.env.NODE_ENV === "development" ? error.message : undefined,
      retryAfter: statusCode === 429 ? 60 : statusCode === 408 ? 10 : undefined,
    });
  }
});

// Demo response function when API quota is exceeded
function getDemoResponse(userMessage) {
  const message = userMessage.toLowerCase();

  if (message.includes("machine learning") || message.includes("ml")) {
    return {
      text: `# Machine Learning - H·ªçc M√°y

## Kh√°i ni·ªám c∆° b·∫£n
Machine Learning (H·ªçc m√°y) l√† m·ªôt nh√°nh c·ªßa Tr√≠ tu·ªá nh√¢n t·∫°o (AI) cho ph√©p m√°y t√≠nh h·ªçc h·ªèi v√† ƒë∆∞a ra d·ª± ƒëo√°n ho·∫∑c quy·∫øt ƒë·ªãnh m√† kh√¥ng c·∫ßn ƒë∆∞·ª£c l·∫≠p tr√¨nh r√µ r√†ng cho t·ª´ng t√°c v·ª• c·ª• th·ªÉ.

## C√°c lo·∫°i Machine Learning
1. **Supervised Learning** (H·ªçc c√≥ gi√°m s√°t): S·ª≠ d·ª•ng d·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c g√°n nh√£n
2. **Unsupervised Learning** (H·ªçc kh√¥ng gi√°m s√°t): T√¨m pattern trong d·ªØ li·ªáu ch∆∞a g√°n nh√£n  
3. **Reinforcement Learning** (H·ªçc tƒÉng c∆∞·ªùng): H·ªçc th√¥ng qua ph·∫ßn th∆∞·ªüng v√† ph·∫°t

## Code Python ƒë∆°n gi·∫£n - Linear Regression

\`\`\`python
# Import th∆∞ vi·ªán c·∫ßn thi·∫øt
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split

# T·∫°o d·ªØ li·ªáu m·∫´u
np.random.seed(42)
X = np.random.rand(100, 1) * 10  # 100 ƒëi·ªÉm d·ªØ li·ªáu
y = 2 * X.ravel() + 1 + np.random.randn(100) * 2  # y = 2x + 1 + noise

# Chia d·ªØ li·ªáu th√†nh t·∫≠p hu·∫•n luy·ªán v√† ki·ªÉm tra
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# T·∫°o model Linear Regression
model = LinearRegression()

# Hu·∫•n luy·ªán model
model.fit(X_train, y_train)

# D·ª± ƒëo√°n
y_pred = model.predict(X_test)

# ƒê√°nh gi√° model
from sklearn.metrics import mean_squared_error, r2_score
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"Mean Squared Error: {mse:.2f}")
print(f"R¬≤ Score: {r2:.2f}")
print(f"H·ªá s·ªë: {model.coef_[0]:.2f}")
print(f"Intercept: {model.intercept_:.2f}")

# V·∫Ω bi·ªÉu ƒë·ªì
plt.figure(figsize=(10, 6))
plt.scatter(X_test, y_test, color='blue', alpha=0.6, label='D·ªØ li·ªáu th·ª±c')
plt.plot(X_test, y_pred, color='red', linewidth=2, label='D·ª± ƒëo√°n')
plt.xlabel('X')
plt.ylabel('y')
plt.title('Linear Regression Demo')
plt.legend()
plt.show()
\`\`\`

## ·ª®ng d·ª•ng th·ª±c t·∫ø
- D·ª± ƒëo√°n gi√° nh√†
- Ph√¢n lo·∫°i email spam
- Nh·∫≠n d·∫°ng h√¨nh ·∫£nh
- G·ª£i √Ω s·∫£n ph·∫©m

*L∆∞u √Ω: ƒê√¢y l√† response demo do API ƒë√£ ƒë·∫°t gi·ªõi h·∫°n. H√£y th·ª≠ l·∫°i sau ho·∫∑c li√™n h·ªá ƒë·ªÉ c·∫≠p nh·∫≠t API key.*`,
    };
  }

  return {
    text: `Xin l·ªói, API hi·ªán t·∫°i ƒë√£ ƒë·∫°t gi·ªõi h·∫°n quota. ƒê√¢y l√† response demo.

ƒê·ªÉ gi·∫£i quy·∫øt v·∫•n ƒë·ªÅ n√†y:
1. Ch·ªù quota reset (th∆∞·ªùng l√† 24h)
2. C·∫≠p nh·∫≠t API key m·ªõi
3. N√¢ng c·∫•p plan API

C√¢u h·ªèi c·ªßa b·∫°n: "${userMessage}"

*Response n√†y ƒë∆∞·ª£c t·∫°o t·ª± ƒë·ªông khi API kh√¥ng kh·∫£ d·ª•ng.*`,
  };
}

// For local development
if (process.env.NODE_ENV !== "production") {
  app.listen(PORT, () => {
    console.log(`‚úÖ Server running at http://localhost:${PORT}`);
  });
}

// Export for Vercel serverless
export default app;
